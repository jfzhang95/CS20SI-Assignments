{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple TensorFlow exercises\n",
    "You should thoroughly test your code\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem1: Ops exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  0.0323758\n",
      "y:  0.353652\n",
      "1a result:  0.386028\n",
      "\n",
      "x:  -0.490113\n",
      "y:  0.124855\n",
      "1b result:  -0.365258\n",
      "\n",
      "x:  [[ 0 -2 -1]\n",
      " [ 0  1  2]]\n",
      "y:  [[0 0 0]\n",
      " [0 0 0]]\n",
      "1c result:  [[ True False False]\n",
      " [ True False False]]\n",
      "\n",
      "1d indices:  [[ 2]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [11]\n",
      " [14]\n",
      " [16]\n",
      " [18]]\n",
      "1d elements:  [[ 31.19073486]\n",
      " [ 30.97266006]\n",
      " [ 38.08450317]\n",
      " [ 34.94445419]\n",
      " [ 34.45999146]\n",
      " [ 36.01657104]\n",
      " [ 30.20379066]\n",
      " [ 33.71149445]\n",
      " [ 36.05556488]]\n",
      "\n",
      "1e result:  [[1 0 0 0 0 0]\n",
      " [0 2 0 0 0 0]\n",
      " [0 0 3 0 0 0]\n",
      " [0 0 0 4 0 0]\n",
      " [0 0 0 0 5 0]\n",
      " [0 0 0 0 0 6]]\n",
      "\n",
      "matrix:  [[ 0.37541512 -0.1299666  -0.31316665  1.63839245 -0.60459846 -1.35199022\n",
      "   0.48886666 -1.57846987 -0.69738352 -1.36678708]\n",
      " [-1.0150975  -0.23947066  0.21490736  1.44574368 -1.41944253 -0.83826554\n",
      "  -1.0676074  -0.08227249  0.88546157 -0.45222366]\n",
      " [-0.76473379 -0.57149118 -0.25639609 -3.09495282 -1.27446663  0.26178613\n",
      "   0.45012856 -0.34655464 -1.37166035 -0.0285361 ]\n",
      " [-1.40588021 -0.22753894  1.67243361  0.55112636  2.08001351  0.4491986\n",
      "   0.42447579 -0.15572378 -0.10738699 -0.65846896]\n",
      " [-0.46443492 -1.17536473  0.41900435  0.15625857 -0.32724056 -1.68608677\n",
      "  -1.23098266 -0.00504803  0.52558702 -1.05732942]\n",
      " [-0.30769688  0.08333594 -0.51406938  0.90155107  0.0404407   0.28220329\n",
      "  -0.86445338  2.03112102 -0.22852187  0.64529788]\n",
      " [ 1.45411193 -2.00175691 -0.33188498  1.40589666  0.81894827 -1.46736646\n",
      "   0.27462804  0.45351669 -0.38560602 -1.48650002]\n",
      " [ 0.74367946  0.31043464 -0.60683274  0.22659916 -0.06714934 -1.26200414\n",
      "   0.23206359 -1.41205776 -0.90306669 -0.04008129]\n",
      " [ 1.18528545 -1.67404318 -2.26183605 -1.54032993  0.06560792 -1.21781969\n",
      "   1.46825016 -0.69576454 -0.34530938  2.20859265]\n",
      " [ 0.59084183  0.20583804 -1.525002   -0.80885005 -0.95533037 -1.3005662\n",
      "  -0.2955496   0.09710146 -0.10400511  0.0757855 ]]\n",
      "1f result:  -114.673\n",
      "\n",
      "1g result:  Unique(y=array([ 5,  2,  3, 10,  6,  4,  1,  0,  9], dtype=int32), idx=array([0, 1, 2, 0, 3, 4, 1, 2, 5, 1, 6, 6, 7, 8], dtype=int32))\n",
      "\n",
      "average of all elements in (x-y) -0.0128316\n",
      "1h result:  2.06855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 1a: Create two random 0-d tensors x and y of any distribution.\n",
    "# Create a TensorFlow object that returns x + y if x < y, and x - y otherwise.\n",
    "# Hint: look up tf.cond()\n",
    "# I do the first problem for you\n",
    "###############################################################################\n",
    "\n",
    "x = tf.random_uniform([])  # Empty array as shape creates a scalar.\n",
    "y = tf.random_uniform([])\n",
    "out = tf.cond(tf.less(x, y), lambda: tf.add(x, y), lambda: tf.subtract(x, y))\n",
    "with tf.Session() as sess:\n",
    "    x, y, result = sess.run([x, y, out])\n",
    "    print(\"x: \",x)\n",
    "    print(\"y: \", y)\n",
    "    print(\"1a result: \", result)\n",
    "    print()\n",
    "\n",
    "###############################################################################\n",
    "# 1b: Create two 0-d tensors x and y randomly selected from -1 and 1.\n",
    "# Return x + y if x < y, x - y if x > y, 0 otherwise.\n",
    "# Hint: Look up tf.case().\n",
    "###############################################################################\n",
    "\n",
    "x = tf.random_uniform([], -1, 1, dtype=tf.float32)\n",
    "y = tf.random_uniform([], -1, 1, dtype=tf.float32)\n",
    "\n",
    "def f1(): return tf.add(x, y)\n",
    "def f2(): return tf.subtract(x, y)\n",
    "def f3(): return tf.constant(0.0)\n",
    "\n",
    "out = tf.case({tf.less(x, y): f1, tf.greater(x, y): f2}, default=f3, exclusive=True)\n",
    "with tf.Session() as sess:\n",
    "    x, y, result = sess.run([x, y, out])\n",
    "    print(\"x: \",x)\n",
    "    print(\"y: \", y)\n",
    "    print(\"1b result: \", result)\n",
    "    print()\n",
    "\n",
    "###############################################################################\n",
    "# 1c: Create the tensor x of the value [[0, -2, -1], [0, 1, 2]] \n",
    "# and y as a tensor of zeros with the same shape as x.\n",
    "# Return a boolean tensor that yields Trues if x equals y element-wise.\n",
    "# Hint: Look up tf.equal().\n",
    "###############################################################################\n",
    "\n",
    "x = tf.constant([[0, -2, -1], [0, 1, 2]])\n",
    "y = tf.zeros_like(x)\n",
    "out = tf.equal(x, y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x, y, result = sess.run([x, y, out])\n",
    "    print(\"x: \", x)\n",
    "    print(\"y: \", y)\n",
    "    print(\"1c result: \", result)\n",
    "    print()\n",
    "    \n",
    "###############################################################################\n",
    "# 1d: Create the tensor x of value \n",
    "# [29.05088806,  27.61298943,  31.19073486,  29.35532951,\n",
    "#  30.97266006,  26.67541885,  38.08450317,  20.74983215,\n",
    "#  34.94445419,  34.45999146,  29.06485367,  36.01657104,\n",
    "#  27.88236427,  20.56035233,  30.20379066,  29.51215172,\n",
    "#  33.71149445,  28.59134293,  36.05556488,  28.66994858].\n",
    "# Get the indices of elements in x whose values are greater than 30.\n",
    "# Hint: Use tf.where().\n",
    "# Then extract elements whose values are greater than 30.\n",
    "# Hint: Use tf.gather().\n",
    "###############################################################################\n",
    "\n",
    "x = tf.constant([29.05088806,  27.61298943,  31.19073486,  29.35532951,\n",
    "                 30.97266006,  26.67541885,  38.08450317,  20.74983215,\n",
    "                 34.94445419,  34.45999146,  29.06485367,  36.01657104,\n",
    "                 27.88236427,  20.56035233,  30.20379066,  29.51215172,\n",
    "                 33.71149445,  28.59134293,  36.05556488,  28.66994858])\n",
    "\n",
    "indices = tf.where(condition=x>30)\n",
    "result = tf.gather(params=x, indices=indices)\n",
    "with tf.Session() as sess:\n",
    "    indices, result = sess.run([indices, result])\n",
    "    print(\"1d indices: \", indices)\n",
    "    print(\"1d elements: \", result)\n",
    "    print()\n",
    "###############################################################################\n",
    "# 1e: Create a diagnoal 2-d tensor of size 6 x 6 with the diagonal values of 1,\n",
    "# 2, ..., 6\n",
    "# Hint: Use tf.range() and tf.diag().\n",
    "###############################################################################\n",
    "\n",
    "out = tf.diag(tf.range(1,7))\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(out)\n",
    "    print(\"1e result: \", result)\n",
    "    print()\n",
    "\n",
    "###############################################################################\n",
    "# 1f: Create a random 2-d tensor of size 10 x 10 from any distribution.\n",
    "# Calculate its determinant.\n",
    "# Hint: Look at tf.matrix_determinant().\n",
    "###############################################################################\n",
    "\n",
    "x = tf.random_normal(shape=(10, 10))\n",
    "out = tf.matrix_determinant(x)\n",
    "with tf.Session() as sess:\n",
    "    matrix, result = sess.run([x, out])\n",
    "    print(\"matrix: \", matrix)\n",
    "    print(\"1f result: \", result)\n",
    "    print()\n",
    "###############################################################################\n",
    "# 1g: Create tensor x with value [5, 2, 3, 5, 10, 6, 2, 3, 4, 2, 1, 1, 0, 9].\n",
    "# Return the unique elements in x\n",
    "# Hint: use tf.unique(). Keep in mind that tf.unique() returns a tuple.\n",
    "###############################################################################\n",
    "\n",
    "x = tf.constant([5, 2, 3, 5, 10, 6, 2, 3, 4, 2, 1, 1, 0, 9])\n",
    "out = tf.unique(x)\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(out)\n",
    "    print(\"1g result: \", result)\n",
    "    print()\n",
    "###############################################################################\n",
    "# 1h: Create two tensors x and y of shape 300 from any normal distribution,\n",
    "# as long as they are from the same distribution.\n",
    "# Use tf.less() and tf.select() to return:\n",
    "# - The mean squared error of (x - y) if the average of all elements in (x - y)\n",
    "#   is negative, or\n",
    "# - The sum of absolute value of all elements in the tensor (x - y) otherwise.\n",
    "# Hint: see the Huber loss function in the lecture slides 3.\n",
    "###############################################################################\n",
    "\n",
    "x = tf.random_normal([300])\n",
    "y = tf.random_normal([300])\n",
    "\n",
    "residual = tf.reduce_mean(x - y)\n",
    "mse = tf.reduce_mean(tf.square(x - y))\n",
    "sav = tf.reduce_sum(tf.abs(x - y))\n",
    "condition = tf.less(residual, 0)\n",
    "out = tf.cond(condition, lambda: mse, lambda: sav)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    residual, result = sess.run([residual, out])\n",
    "    print(\"average of all elements in (x-y)\", residual)\n",
    "    print(\"1h result: \", result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem2: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data......\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Done!\n",
      "Total loss in iter0:  986.721645832\n",
      "Total loss in iter1:  983.320941925\n",
      "Total loss in iter2:  968.140684128\n",
      "Total loss in iter3:  875.077893853\n",
      "Total loss in iter4:  582.860703468\n",
      "Total loss in iter5:  385.174012065\n",
      "Total loss in iter6:  310.377721101\n",
      "Total loss in iter7:  263.057091296\n",
      "Total loss in iter8:  231.716411233\n",
      "Total loss in iter9:  211.488890141\n",
      "Total loss in iter10:  196.65275231\n",
      "Total loss in iter11:  185.013164118\n",
      "Total loss in iter12:  176.04174915\n",
      "Total loss in iter13:  167.040936559\n",
      "Total loss in iter14:  161.032677472\n",
      "Total loss in iter15:  155.041350782\n",
      "Total loss in iter16:  149.426403373\n",
      "Total loss in iter17:  144.199834034\n",
      "Total loss in iter18:  138.827529088\n",
      "Total loss in iter19:  134.833240852\n",
      "Total loss in iter20:  129.375457615\n",
      "Total loss in iter21:  126.134714201\n",
      "Total loss in iter22:  122.1577757\n",
      "Total loss in iter23:  118.057299599\n",
      "Total loss in iter24:  114.297893032\n",
      "Total loss in iter25:  111.234215848\n",
      "Total loss in iter26:  107.641163133\n",
      "Total loss in iter27:  103.881650969\n",
      "Total loss in iter28:  101.73033689\n",
      "Total loss in iter29:  98.2029024586\n",
      "Total loss in iter30:  95.4596293047\n",
      "Total loss in iter31:  93.3374552429\n",
      "Total loss in iter32:  90.4624845982\n",
      "Total loss in iter33:  88.3020281792\n",
      "Total loss in iter34:  86.9051454291\n",
      "Total loss in iter35:  84.388921991\n",
      "Total loss in iter36:  82.1993982419\n",
      "Total loss in iter37:  80.9179061502\n",
      "Total loss in iter38:  78.2997713462\n",
      "Total loss in iter39:  76.9761406779\n",
      "Total loss in iter40:  75.2489500791\n",
      "Total loss in iter41:  73.5367963314\n",
      "Total loss in iter42:  72.2635310143\n",
      "Total loss in iter43:  70.3966002353\n",
      "Total loss in iter44:  69.5930593163\n",
      "Accuracy 0.9498\n"
     ]
    }
   ],
   "source": [
    "# step 1: read in data\n",
    "print(\"Loading data......\")\n",
    "MNIST = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(\"Done!\")\n",
    "# step 2: define parameters for the model\n",
    "LEARNING_RATE = 1e-2\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 45\n",
    "H1_UNITS = 512\n",
    "H2_UNITS = 380\n",
    "KEEP_PROB = 0.5\n",
    "\n",
    "# step 3: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9.\n",
    "# each label is one hot vector.\n",
    "\n",
    "X = tf.placeholder(tf.float32, [BATCH_SIZE, 784])\n",
    "Y = tf.placeholder(tf.float32, [BATCH_SIZE, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# step 4: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal(shape=[784, H1_UNITS], stddev=0.01), name=\"W1\")\n",
    "b1 = tf.Variable(tf.zeros([1, H1_UNITS]), name=\"b1\")\n",
    "h1 = tf.nn.relu(tf.matmul(X, W1)+b1)\n",
    "h1_drop = tf.nn.dropout(h1, KEEP_PROB)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal(shape=[H1_UNITS, H2_UNITS], stddev=0.01), name=\"W2\")\n",
    "b2 = tf.Variable(tf.zeros([1, H2_UNITS]), name=\"b2\")\n",
    "h2 = tf.nn.relu(tf.matmul(h1_drop, W2)+b2)\n",
    "\n",
    "W_out = tf.Variable(tf.random_normal(shape=[H2_UNITS, 10], stddev=0.01), name=\"W_out\")\n",
    "b_out = tf.Variable(tf.zeros([1, 10]), name=\"b_out\")\n",
    "\n",
    "\n",
    "# step 5: predict Y from X and w, b\n",
    "# the model that returns probability distribution of possible label of the image\n",
    "# through the softmax layer\n",
    "# a batch_size x 10 tensor that represents the possibility of the digits\n",
    "\n",
    "logits = tf.matmul(h2, W_out) + b_out\n",
    "\n",
    "# step 6: define loss function\n",
    "# use softmax cross entropy with logits as the loss function\n",
    "# compute mean cross entropy, softmax is applied internally\n",
    "Y_ = tf.nn.softmax(logits)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(Y_), reduction_indices=[1]))\n",
    "\n",
    "# Step 7: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    n_batches = int(MNIST.train.num_examples/BATCH_SIZE)\n",
    "    for i in range(N_EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(BATCH_SIZE)\n",
    "            _, loss = sess.run([optimizer, cross_entropy_loss], feed_dict={X:X_batch, Y:Y_batch})\n",
    "            total_loss += loss\n",
    "        print(\"Total loss in iter%d: \" % i, total_loss)\n",
    "    \n",
    "    # test model\n",
    "    n_batches = int(MNIST.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0.0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, pred = sess.run([optimizer, cross_entropy_loss, Y_], feed_dict={X:X_batch, Y:Y_batch})\n",
    "        correct_preds = tf.equal(tf.argmax(pred,1), tf.argmax(Y_batch,1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    print(\"Accuracy {0}\".format(total_correct_preds / MNIST.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression model to predict whether someone has coronary heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>Present</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>Absent</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>Present</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>Present</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>Present</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
       "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1\n",
       "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n",
       "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0\n",
       "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1\n",
       "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/jesse/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/jesse/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jesse/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data.famhist[data['famhist']=='Present'] = 1\n",
    "data.famhist[data['famhist']=='Absent'] = 0\n",
    "data['chd_0'] = 0\n",
    "data.chd_0[data['chd']==0] = 1.0\n",
    "data.chd[data['chd']==1] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "      <th>chd_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sbp  tobacco   ldl  adiposity famhist  typea  obesity  alcohol  age  chd  \\\n",
       "0  160    12.00  5.73      23.11       1     49    25.30    97.20   52    1   \n",
       "1  144     0.01  4.41      28.61       0     55    28.87     2.06   63    1   \n",
       "2  118     0.08  3.48      32.28       1     52    29.14     3.81   46    0   \n",
       "3  170     7.50  6.41      38.03       1     51    31.99    24.26   58    1   \n",
       "4  134    13.60  3.50      27.78       1     60    25.99    57.34   49    1   \n",
       "\n",
       "   chd_0  \n",
       "0      0  \n",
       "1      0  \n",
       "2      1  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = pd.concat([data.chd, data.chd_0], axis=1)\n",
    "X = data.drop('chd', axis=1).drop('chd_0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.13, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data count:  (401, 9)\n",
      "test data count:  (61, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data count: \", X_train.shape)\n",
    "print(\"test data count: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "N_EPOCH = 100\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [BATCH_SIZE, 9])\n",
    "Y = tf.placeholder(tf.float32, [BATCH_SIZE, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([9, 2]))\n",
    "b = tf.Variable(tf.zeros([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0:0.6856650069355965\n",
      "Average loss epoch 20:0.6125385120511055\n",
      "Average loss epoch 40:0.6023114033043384\n",
      "Average loss epoch 60:0.5969743080437183\n",
      "Average loss epoch 80:0.5937002699822187\n",
      "Accuracy 66.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    sess.run(init)\n",
    "    n_batches = int(len(X_train)/BATCH_SIZE)\n",
    "    for i in range(N_EPOCH):\n",
    "        total_loss = 0.0\n",
    "        for n in range(n_batches):\n",
    "            X_batch = X_train[n*BATCH_SIZE:(n+1)*BATCH_SIZE]\n",
    "            Y_batch = Y_train[n*BATCH_SIZE:(n+1)*BATCH_SIZE]\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X:X_batch, Y:Y_batch})\n",
    "            \n",
    "            total_loss += loss_batch\n",
    "        if i % 20 == 0:\n",
    "            print(\"Average loss epoch {0}:{1}\".format(i, total_loss/n_batches))\n",
    "    \n",
    "    n_batches = int(len(X_test) / BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for n in range(n_batches):\n",
    "        X_batch = X_test[n*BATCH_SIZE:(n+1)*BATCH_SIZE]\n",
    "        Y_batch = Y_test[n*BATCH_SIZE:(n+1)*BATCH_SIZE]\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, y], feed_dict={X: X_batch, Y: Y_batch})\n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch.as_matrix(), 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))  # need numpy.count_nonzero(boolarr) :(\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    " \n",
    "    print('Accuracy {0}%'.format(100 * total_correct_preds / (n_batches * BATCH_SIZE)))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import urllib\n",
    "from urllib import request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "def build_vocab(words, vocab_size):\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words \"\"\"\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)]\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    # >> 原来的写法\n",
    "    index = 0\n",
    "    with open('processed/vocab_1000.tsv', \"w\") as f:\n",
    "        # f.write(\"Name\\n\")\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = index\n",
    "            if index < 1000:\n",
    "                f.write(word + \"\\n\")\n",
    "            index += 1\n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "words = read_data('text8.zip')\n",
    "print('data size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(words, vocab_size):\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words \"\"\"\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)]\n",
    "    count.extend(collection.Counter(words).most_common(vocab_size - 1))\n",
    "\n",
    "    # >> 原来的写法\n",
    "    index = 0\n",
    "    with open('processed/vocab_1000.tsv', \"w\") as f:\n",
    "        # f.write(\"Name\\n\")\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = index\n",
    "            if index < 1000:\n",
    "                f.write(word + \"\\n\")\n",
    "            index += 1\n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_words_to_index(words, dictionary):"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
